---
title: "Vertically_Poisson_Regression"
output: html_document
---


```{r}

Poisson_data_generete <- function(X, true_coefficient){
  
  n_X <- nrow(X)
  X <- as.matrix(X)
  
  beta_true <- true_coefficient
  
  eta <- as.matrix(X) %*% as.matrix(beta_true)
  Y_mean <- exp(eta)
  
  # Generate Y values according to Y_mean 
  
  Y <- rpois(n = n_X, lambda = Y_mean) 
  
  sim_data <- cbind(Y, X)

  return(sim_data)
}

# Example

n <- 400

set.seed(115)

x1 <- runif(n = n, min = 0, max = 2)
x2 <- rexp(n = n, rate = 1)
x3 <- rbinom(n = n, size = 5, prob = 0.1)
x3 <- as.numeric(x3)
beta <- c(3, 1, 2)

X <- data.frame(
  "X1" <- x1,
  "X2" <- x2,
  "X3" <- x3
)

simulation_data <- Poisson_data_generete(X, true_coefficient = beta)
simulation_data <- as.data.frame(simulation_data)
colnames(simulation_data) <- c("Y", "X1", "X2", "X3")
```


```{r}

sim_model <- glm(Y ~ X1 + X2 + X3, family = poisson(link = "log"), data = simulation_data)

summary(sim_model)
```
#Likelihood, Gradient and Hessian
```{r}

poisson_dual_likelihood <- function(Y, X, lambda, u){
  X <- as.matrix(X)
  
  
  m <- nrow(X)
  XXT <- X %*% t(X) 
  
  return(  (sum((u - Y)*((log(Y - u)) - 1))/m - (t(u) %*% XXT %*% u)/(2*m*m*lambda))[1,1] )
}


poisson_dual_grad <- function(Y, X, lambda, u){
  
  m <- nrow(X)
  
  X <- as.matrix(X)
  XXT <- X %*% t(X)
  
  log_term <- log(Y - u)
  regularization <- (XXT %*% u)/(m*m*lambda)
  
  g <- (log_term)/m - regularization
  
  
  return( g )
  
}

 
poisson_dual_Hess <- function(Y, X, lambda, u){
  X <- as.matrix(X)
  m <- nrow(X)
  XXT <- X %*% t(X) 
  
  main_h <- (1/(u - Y))/m
  diag_main_h <- matrix(0, nrow = m, ncol = m) 
  diag(diag_main_h) <- main_h
  
  H <- diag_main_h - XXT/(m*m*lambda)
  
  return( H )
}


```


Newton's Method
```{r}
newton_Poisson_optim_dual <- function(f = poisson_dual_likelihood, 
                                    grad = poisson_dual_grad, 
                                    hess = poisson_dual_Hess
                                    , x0 , tol = 1e-2, max_iter = 10, 
                                    data_set, response_idx){
  x <- x0
  iter <- 0
  converged <- FALSE
  Y <- data_set[,response_idx]
  X <- data_set[, -response_idx]
  
  for (i in 1:max_iter) {
    g <- -poisson_dual_grad(Y, X, lambda = 0.2, u = x)
    H <- -poisson_dual_Hess(Y, X, lambda = 0.2, u = x)
    
    # Newton step: solve H %*% step = -g
    step <- solve(H) %*% (-g)
    
    # Handle cases where Hessian might not be invertible
   # if (inherits(step, "try-error")) {
   #   warning("Hessian is singular at iteration ", i)
   #   break
    }
    
    x_new <- x + step
    iter <- i
    
    # Check convergence (using L2 norm of gradient)
    if ( sqrt(sum(g^2)) < 1e-2 ) {
      converged <- TRUE
      break
    }
    
    x <- x_new
  
  
  list(par = x, 
       value = -poisson_dual_likelihood(Y, X, lambda = 0.2, u = x), 
       iterations = iter, 
       converged = converged,
       coefficient_estimate = (t(data_set[,-response_idx]) %*% x)/(nrow(data_set)*0.2))
  

}
```

```{r}
newton_Poisson_optim_dual(x0 = rep(0, nrow(X)), data_set = Poisson_data_generete(X, true_coefficient = beta), response_idx = 1 )
```

Projected Gradient Ascent 
```{r}
poisson_dual_gradient_ascent <- function(Y, X, lambda, u0 = NULL,
                             eps = 1e-8,
                             step0 = 1.0,
                             max_iter = 50000 ,
                             tol = 1e-6,
                             backtrack_beta = 0.5,
                             armijo_c = 1e-4,
                             verbose = TRUE){

  Y <- as.numeric(Y)
  X <- as.matrix(X)
  m <- nrow(X)

  if (length(Y) != m) stop("length(Y) must equal nrow(X).")
  if (!is.finite(lambda) || lambda <= 0) stop("lambda must be > 0.")

  # projection to the feasible set: u <= Y - eps (elementwise)
  proj_u <- function(u) pmin(u, Y - eps)

  # initialize u
  if (is.null(u0)) {
    u <- Y - 1.0  # a safe default if Y >= 1; we'll project anyway
  } else {
    u <- as.numeric(u0)
    if (length(u) != m) stop("length(u0) must equal length(Y).")
  }
  u <- proj_u(u)

  # helper: safe objective (returns -Inf if out of domain)
  f_safe <- function(u){
    if (any(Y - u <= 0) || any(!is.finite(u))) return(-Inf)
    poisson_dual_likelihood(Y, X, lambda, u)
  }

  f <- f_safe(u)
  if (!is.finite(f)) stop("Initial u is infeasible (Y - u must be > 0).")

  history <- data.frame(iter = integer(0), obj = numeric(0), grad_norm = numeric(0), step = numeric(0))

  for (iter in 1:max_iter){
    g <- poisson_dual_grad(Y, X, lambda, u)
    if (any(!is.finite(g))) stop("Gradient produced non-finite values; check feasibility and inputs.")

    gnorm <- sqrt(sum(g^2))
    if (gnorm < tol) {
      if (verbose) message(sprintf("Converged at iter %d: grad_norm=%.3e, obj=%.10f", iter, gnorm, f))
      history <- rbind(history, data.frame(iter=iter, obj=f, grad_norm=gnorm, step=0))
      break
    }

    step <- step0
    # ascent direction is +g
    # Armijo-style condition for ascent:
    # f(u_new) >= f(u) + c * step * ||g||^2
    repeat {
      u_new <- proj_u(u + step * g)  # projection enforces Y - u_new >= eps
      f_new <- f_safe(u_new)

      if (is.finite(f_new) && (f_new >= f + armijo_c * step * (gnorm^2))) {
        break
      }

      step <- step * backtrack_beta
      if (step < 1e-16) {
        if (verbose) message(sprintf("Step size collapsed at iter %d. Stopping.", iter))
        u_new <- u
        f_new <- f
        break
      }
    }

    u <- u_new
    f <- f_new



  }

  list(u = u, objective = f)
}



```

```{r}
poisson_sim_u_opt <- poisson_dual_gradient_ascent(Y = simulation_data$Y, X = X, lambda = 0.02, u0 = rep(0, nrow(X)))$u
t(X) %*% poisson_sim_u_opt / (nrow(X)  * 0.02)
```



```{r}
# What if we run Poisson model for every single feature

feature_1_only_model <- glm(Y ~ X1, family = poisson(link = "log"), data = simulation_data)
feature_1_only_model$coefficients

feature_2_only_model <- glm(Y ~ X2, family = poisson(link = "log"), data = simulation_data)
feature_2_only_model$coefficients

feature_3_only_model <- glm(Y ~ X3, family = poisson(link = "log"), data = simulation_data)
feature_3_only_model$coefficients



```

```{r}
# Bootstrapping for confidence interval


beta_estimate_dual <- newton_Poisson_optim_dual(x0 = rep(0, nrow(X)), data_set = Poisson_data_generete(X, true_coefficient = beta), response_idx = 1 )$coefficient_estimate

set.seed(115)


n_boot <- 50

# Store bootstrap samples in a list
bootstrap_samples <- vector("list", n_boot)

# Perform resampling

for (i in seq_len(n_boot)) {
  bootstrap_samples[[i]] <- simulation_data[sample(nrow(simulation_data), replace = TRUE), ]
}


beta_estimate_dual_bootstrap <-  sapply(bootstrap_samples, function(df) {
  bs_coeff <- newton_Poisson_optim_dual(data_set = df, x0 = rep(0, nrow(df)), response_idx = 1)$coefficient_estimate
} )


var_bootstrap <- apply(beta_estimate_dual_bootstrap, MARGIN = 1, FUN = var)

diff_bootstrap <- sweep(beta_estimate_dual_bootstrap, 1, beta_estimate_dual, FUN = "-")
#standardized_diff <- sweep(standardized_diff, 1, sqrt(boot_variance), FUN = "/")


```


Monthly grouped numbers of cases v.s. all chemicals

```{r}
library(readxl)
library(tidyverse)

nmda <- read_excel("D:/Research_Vertically_Federated_Learning/NMDA_with_population_sex_ratio_copy.xlsx")
```

```{r}
source("NMDA_preprocessing.R")

df_model <- summary_by_onset %>%
  dplyr::select(-onset_month)

y_n_cases <- df_model$n_cases
X_chemicals <- model.matrix(n_cases ~ . , data = df_model)[, -1]

#poisson_sim_u_opt_nmda <- poisson_dual_gradient_ascent(Y = y_n_cases, X = X_chemicals, lambda = 0.02, u0 = rep(0, nrow(X_chemicals)))$u
#t(X_chemicals) %*% poisson_sim_u_opt_nmda / (nrow(X_chemicals)  * 0.02)
```

mRS


```{r}
y_mRS <- nmda$mRS
y_mRS <- as.numeric(y_mRS)

X_design <- nmda %>%
  select(Age, Seizures, CO_24h_M0)

X_design <- as.matrix(X_design)
X_design <- apply(X_design, 2, as.numeric)



idx_keep <- apply(X_design, 1, function(r) all(is.finite(r)))
sum(!idx_keep)   # how many rows dropped

X_design <- X_design[idx_keep, , drop = FALSE]
y_mRS <- y_mRS[idx_keep]




mRS_opt_u <- poisson_dual_gradient_ascent(Y = y_mRS, X = X_design, lambda = 0.02, max_iter = 1e5)$u

mRS_dual_beta <- t(X_design) %*% mRS_opt_u / (nrow(X_design)  * 0.02)
```

```{r}
glm(
  Y ~ . - 1,
  data   = data.frame(Y = y_mRS, X_design),
  family = poisson(link = "log")
)
```












